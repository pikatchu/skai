module alias AF = ArrayFire;

mutable class Env(
  data: mutable Map<String, AF.Array> = mutable Map[],
  mutable currentId: Int = 0,
) {
  mutable private fun genSym(): String {
    this.!currentId = this.currentId + 1;
    "w_" + this.currentId.toString()
  }

  mutable fun parameter(weights: AF.Array, name: String = ""): Tensor {
    n = if (name == "") {
      this.genSym()
    } else {
      name
    };
    this.data![n] = weights;
    Tensor.Param(n)
  }
}

mutable base class Model {
  children =
  | Sigmoid()
  | Seq(Vector<Model>)
  | Linear(Tensor)

  untracked fun forward(input: Tensor): Tensor
  | Sigmoid() -> Tensor.Sigmoid(input)
  | Linear(w) ->
    Tensor.MatMul(Tensor.Transpose(w), input)
  | Seq(v) ->
    acc = input;
    v.each(x -> !acc = x.forward(acc));
    acc
}

untracked fun makeMLP(dims: Vector<Int>, env: mutable Env): Model {
  seq = mutable Vector<Model>[];
  for (i in Range(0, dims.size() - 1)) {
    w_i = env.parameter(AF.randu(dims[i], dims[i+1]), `w_${i}`);

    seq.push(Linear(w_i));
    seq.push(Sigmoid());
  };
  Seq(freeze(seq))
}

base class Loss {
  children =
  | MSELoss()

  fun compute(yiHat: Tensor, yi: Tensor): Tensor
  | MSELoss() -> res = yiHat - yi;
    Tensor.MatMul(Tensor.Transpose(res), res)
}

base class Optimizer {
  children =
  | SGD(lr: Float)

  untracked fun step(loss: Tensor, env: mutable Env,
                     data: Array<(AF.Array, AF.Array)>): void
  | SGD(lr) -> {
    all_grads = mutable Map<String, AF.Array>[];
    data.each(d -> {
      (xi, yi) = d;
      env.data!["input"] = xi;
      env.data!["output"] = yi;
      grads = loss.backward(Tensor.Constant(1.0, 1));
      fenv = freeze(env.data);
      fenv.each((k, _) -> {
        // TODO: Distinguish between inputs/constants/parameters at Tensor level.
        if (k != "input" && k != "output") {
          if (all_grads.containsKey(k)) {
            all_grads![k] = all_grads[k] + grads[k].eval(fenv);
          } else {
            all_grads![k] = grads[k].eval(fenv)
          }
        };
      })
    });
    all_grads.each((k, w) -> {
      env.data![k] = env.data[k] - AF.mult_fa(lr / all_grads.size().toFloat(), w)
    });
  }
}

untracked fun buildSimpleExample(): void {
  data = Range(0, 20).map(_ -> { x = AF.randu(1); (x, AF.sin(x))});

  eval = Range(0, 5).map(_ -> { x = AF.randu(1); (x, AF.sin(x)) });

  env = mutable Env();
  lr = 0.1;
  _opt = SGD(lr);

  nn = makeMLP(Vector[1, 10, 1], env);

  for (epoch in Range(0, 1000)) {
    all_grads = mutable Map<String, AF.Array>[];
    data.each(d -> {
      (xi, yi) = d;
      model = nn.forward(Tensor.Input(xi));
      loss = MSELoss().compute(model, Tensor.Input(yi));
      //print_string(loss.dims(env));
      grads = loss.backward(Tensor.Constant(1.0, 1));
      fenv = freeze(env.data);
      grads.each((k, _) -> {
        if (all_grads.containsKey(k)) {
          all_grads![k] = all_grads[k] + grads[k].eval(fenv);
        } else {
          all_grads![k] = grads[k].eval(fenv)
        }
      })
    });
    all_grads.each((k, w) -> {
      env.data![k] = env.data[k] - AF.mult_fa(lr / all_grads.size().toFloat(), w)
    });

    avg_loss = eval.map(d -> {
      (xi, yi) = d;
      MSELoss()
      .compute(nn.forward(Tensor.Input(xi)), Tensor.Input(yi))
      .eval(freeze(env.data))
      .scalar()
    }).sumFloat() / eval.size().toFloat();
    print_string(`Epoch #${epoch}: \tAverage loss: ${avg_loss}`);

    // opt.step(loss, env, data);
  };
}
