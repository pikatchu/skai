module alias AF = ArrayFire;

untracked fun makeMLP(dims: Vector<Int>, env: mutable Env): Model {
  seq = mutable Vector<Model>[];
  for (i in Range(0, dims.size() - 1)) {
    w_i = env.parameter(AF.randu(dims[i], dims[i+1]), `w_${i}`);

    seq.push(Linear(w_i));
    seq.push(Sigmoid());
  };
  Seq(freeze(seq))
}


mutable class Env(
  data: mutable Map<String, AF.Array> = mutable Map[],
  mutable currentId: Int = 0,
) {
  mutable private fun genSym(): String {
    this.!currentId = this.currentId + 1;
    "w_" + this.currentId.toString()
  }

  mutable fun parameter(weights: AF.Array, name: String = ""): Tensor {
    n = if (name == "") {
      this.genSym()
    } else {
      name
    };
    this.data![n] = weights;
    Tensor.Param(n)
  }

  readonly fun print(): void {
    for ((k, w) in this.data.items()) {
      print_string(k);
      AF.print(w);
    }
  }
}

mutable base class Model {
  children =
  | Sigmoid()
  | Seq(Vector<Model>)
  | Linear(Tensor)

  fun forward(input: Tensor): Tensor
  | Sigmoid() -> Tensor.Sigmoid(input)
  | Linear(w) ->
    Tensor.MatMul(Tensor.Transpose(w), input)
  | Seq(v) ->
    acc = input;
    v.each(x -> !acc = x.forward(acc));
    acc
}

base class Optimizer {
  children =
  | SGD{lr: Float}

  untracked fun step(loss_fun: (Tensor, Tensor) -> Tensor,
                   data: Sequence<(AF.Array, AF.Array)>,
                   env: mutable Env): void
  | SGD{lr} ->
    all_grads = mutable Map<String, AF.Array>[];
    for (d in data) {
      (xi, yi) = d;
      loss = loss_fun(Tensor.Input(xi), Tensor.Input(yi));
      grads = loss.backward(Tensor.Constant(1.0, 1));
      fenv = freeze(env.data);
      for ((k, w) in grads.items()) {
        if (all_grads.containsKey(k)) {
          all_grads![k] = all_grads[k] + w.eval(fenv);
        } else {
          all_grads![k] = w.eval(fenv)
        }
      };
    };
    for ((k, w) in all_grads.items()) {
      env.data![k] = env.data[k] - AF.Const(lr / data.size().toFloat()) * w
    };
}

mutable class Trainer {
  model: Model,
  loss_fun: (Tensor, Tensor) -> Tensor,
  opt: Optimizer,
  env: mutable Env
} {
  mutable fun loss(data: Sequence<(AF.Array, AF.Array)>): Float {
    data.map(d -> {
      (xi, yi) = d;
      this.loss_fun(this.model.forward(Tensor.Input(xi)), Tensor.Input(yi))
      .eval(freeze(this.env.data))
      .scalar()
    }).sumFloat() / data.size().toFloat();
  }

  untracked mutable fun step(data: Sequence<(AF.Array, AF.Array)>): void {
    this.opt.step((xi, yi) -> { this.loss_fun(this.model.forward(xi), yi) },
                  data, this.env);
  }
}

untracked fun buildSimpleExample(): void {
  data = Range(0, 20).map(_ -> { x = AF.randu(1); (x, AF.sin(x)) });
  eval = Range(0,  5).map(_ -> { x = AF.randu(1); (x, AF.sin(x)) });

  params = mutable Env();
  w_0 = params.parameter(AF.randu(1, 10));
  w_1 = params.parameter(AF.randu(10, 1));
  model = Seq(Vector[Linear(w_0),
                     Sigmoid(),
                     Linear(w_1),
                     Sigmoid()]);

  trainer = mutable Trainer{
    model => model,
    loss_fun => Loss.mse,
    opt => SGD{lr => 1.0},
    env => params
  };

  for (epoch in Range(0, 1000)) {
    trainer.step(data);
    print_string(`Epoch #${epoch}\t  Loss: ${trainer.loss(eval)}\t (train loss: ${trainer.loss(data)})`)
  };
}
