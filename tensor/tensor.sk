/**
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */
module alias AF = ArrayFire;

module Tensor;

mutable class Params(
  data: mutable Map<String, Tensor> = mutable Map[],
  mutable currentId: .Int = 0
) {
  mutable private fun genSym(): String {
    this.!currentId = this.currentId + 1;

    "w_" + this.currentId.toString()
  }

  mutable fun add(value: AF.Array, name: String = ""): Tensor.Param {
    if (name == "") {
      !name = this.genSym()
    };
    this.data![name] = Tensor.Input(value);

    Tensor.Param(name)
  }

  readonly fun get<T: Show>(name: T): Tensor {
    this.data[name.toString()];
  }

  mutable fun set<T: Show>(name: T, arr: AF.Array): void {
    this.data![name.toString()] = Tensor.Input(arr);
  }

  readonly fun items(): mutable Map.MapItemsIterator<String, Tensor> {
    freeze(this.data).items()
  }

  readonly fun print(): void {
    for ((k, w) in this.data.items()) {
      print_string(k);
      AF.print(w.eval());
    }
  }
}

base class OperationKind {
  children =
  | Multiply()
  | Divide()
  | Add()
  | Sub()
}

class Int(value: .Int) extends Tensor {
  fun *(rhs: Tensor): Tensor {
    ConstantOfInt(this.value, rhs) * rhs
  }

  fun +(rhs: Tensor): Tensor {
    ConstantOfInt(this.value, rhs) + rhs
  }

  fun -(rhs: Tensor): Tensor {
    ConstantOfInt(this.value, rhs) - rhs
  }

  fun /(rhs: Tensor): Tensor {
    ConstantOfInt(this.value, rhs) / rhs
  }

  fun negate(): Tensor {
    !this.value = -this.value;
    this
  }

  fun reciprocal(): Tensor {
    invariant_violation("Reciprocal on Int not supported")
  }
}

base class .Tensor {
  children =
  | Param(String)
  | Input(AF.Array)
  | Constant(Float, .Int)
  | ConstantOfInt(.Int, Tensor)
  | Bind(String, Tensor)
  | Operation(OperationKind, Tensor, Tensor)
  | Negate(Tensor)
  | Reciprocal(Tensor)
  | Exp(Tensor)
  | Sigmoid(Tensor)
  | Sin(Tensor)
  | Cos(Tensor)
  | Tan(Tensor)
  | MatMul(Tensor, Tensor)
  | Transpose(Tensor)

  overridable fun *(rhs: Tensor): Tensor {
    rhs match {
    | Int(n) -> this * ConstantOfInt(n, this)
    | _ -> Operation(Multiply(), this, rhs)
    }
  }

  overridable fun /(rhs: Tensor): Tensor {
    rhs match {
    | Int(n) -> this / ConstantOfInt(n, this)
    | _ -> Operation(Divide(), this, rhs)
    }
  }

  overridable fun +(rhs: Tensor): Tensor {
    rhs match {
    | Int(n) -> this + ConstantOfInt(n, this)
    | _ -> Operation(Add(), this, rhs)
    }
  }

  overridable fun -(rhs: Tensor): Tensor {
    rhs match {
    | Int(n) -> this - ConstantOfInt(n, this)
    | _ -> Operation(Sub(), this, rhs)
    }
  }

  overridable fun negate(): Tensor {
    Negate(this)
  }

  overridable fun reciprocal(): Tensor {
    Reciprocal(this)
  }

  fun dims(params: Map<String, AF.Array>): Vector<.Int>
  | Int(_tensor) -> invariant_violation("Case not handled yet")
  | Param(name) -> if (params.containsKey(name)) {
      Input(params[name]).dims(params)
    } else {
      invariant_violation(`Unknown input ${name}`)
    }
  | Input(arr) -> v = mutable Vector[];
    for (i in Range(0, AF.numDims(arr))) {
      v.push(AF.getDim(arr, i))
    };
    freeze(v)
  | Constant(_, n) -> Vector[n]
  | Bind(_, tensor)
  | ConstantOfInt(_, tensor)
  | Negate(tensor)
  | Reciprocal(tensor)
  | Sin(tensor)
  | Cos(tensor)
  | Tan(tensor)
  | Exp(tensor)
  | Sigmoid(tensor) -> tensor.dims(params)
  | Transpose(tensor) -> d = tensor.dims(params);
    if (d.size() == 1) {
      if (d[0] == 1) {
        Vector[1]
      } else {
        Vector[1, d[0]]
      }
    } else if(d.size() == 2) {
      if (d[0] == 1) {
        Vector[d[1]]
      } else {
        Vector[d[1], d[0]]
      }
    } else {
      invariant_violation("Cannot transpose higher order tensors.")
    }
  | MatMul(u, v) -> du = u.dims(params); dv = v.dims(params);
    if (du.size() > 2 && dv.size() > 2) {
      invariant_violation("Cannot matmul higher order tensors.")
    };
    if ((du.size() == 1 && dv[0] != 1) || (du.size() >= 2 && du[1] != dv[0])) {
      debug(u);
      debug(v);
      invariant_violation(`Dimensions mismatch: ${du} vs ${dv}`)
    };
    if (dv.size() >= 2) {
      Vector[du[0], dv[1]]
    } else {
      Vector[du[0]]
    }
  | Operation(_, lhs, rhs) -> lhsDims = lhs.dims(params); rhsDims = rhs.dims(params);
    if (lhsDims != rhsDims) {
      debug(lhs);
      debug(rhs);
      invariant_violation(`Dimensions mismatch: ${lhsDims} vs ${rhsDims}`)
    } else {
      lhsDims
    }

  static fun addGradient(
    name: String,
    gradient: Tensor,
    acc: mutable Map<String, Tensor>,
  ): void {
    acc![name] = if (acc.containsKey(name)) acc[name] + gradient else gradient
  }

  protected fun buildGradients(
    gradient: Tensor,
    acc: mutable Map<String, Tensor>,
  ): void
  | Int _
  | Constant _
  | ConstantOfInt _
  | Input _ ->
    void
  | Param(name) -> static::addGradient(name, gradient, acc)
  | Bind(name, tensor) ->
    static::addGradient(name, gradient, acc);
    tensor.buildGradients(gradient, acc)
  | Operation(operator, lhs, rhs) ->
    operator match {
    | Multiply() ->
      lhs.buildGradients(gradient * rhs, acc);
      rhs.buildGradients(gradient * lhs, acc)
    | Divide() ->
      rhsRec = rhs.reciprocal();
      lhsGrad = gradient * rhsRec;
      lhs.buildGradients(lhsGrad, acc);
      rhs.buildGradients(-lhsGrad * lhs * rhsRec, acc)
    | Add() ->
      lhs.buildGradients(gradient, acc);
      rhs.buildGradients(gradient, acc)
    | Sub() ->
      lhs.buildGradients(gradient, acc);
      rhs.buildGradients(-gradient, acc)
    }
  | Negate(tensor) -> tensor.buildGradients(-gradient, acc)
  | Reciprocal(tensor) -> res = tensor.reciprocal();
    tensor.buildGradients(-gradient * res * res, acc)
  | Sin(tensor) -> tensor.buildGradients(gradient * Cos(tensor), acc)
  | Cos(tensor) -> tensor.buildGradients(-gradient * Sin(tensor), acc)
  | Tan(tensor) -> tensor.buildGradients(
      gradient * Reciprocal(Cos(tensor) * Cos(tensor)), acc)
  | Exp(tensor) -> tensor.buildGradients(gradient * Exp(tensor), acc)
  | Sigmoid(tensor) -> res = Reciprocal(Int(1) + Exp(-tensor));
    tensor.buildGradients(gradient * res * (Int(1) - res), acc)
  | MatMul(lhs, rhs) ->
    lhs.buildGradients(MatMul(gradient, Transpose(rhs)), acc);
    rhs.buildGradients(MatMul(Transpose(lhs), gradient), acc)
  | Transpose(tensor) -> tensor.buildGradients(Transpose(gradient), acc)

  fun backward(
    _params: readonly Params = Params(),
    gradient: Tensor = ConstantOfInt(1, this)
  ): Params {
    acc = mutable Map[];
    this.buildGradients(gradient, acc);

    Params(freeze(acc))
  }

  @gc
  memoized async fun asyncEval(params: Map<String, Tensor>): ^AF.Array {
    this match {
    | Int _ -> invariant_violation("Bad usage of Tensor.Int")
    | Param(n) ->
      if (!params.containsKey(n)) {
        invariant_violation(`Parameter ${n} undefined`)
      } else {
        await params[n].asyncEval(params)
      }
    | Input(arr) -> arr
    | Bind(_, tensor) -> await tensor.asyncEval(params)
    | ConstantOfInt(value, tensor) ->
      AF.constant_of_int(value, await tensor.asyncEval(params))
    | Constant(value, dims) -> AF.constant(value, dims)
    | Operation(operator, lhsTree, rhsTree) ->
      lhs = await lhsTree.asyncEval(params);
      rhs = await rhsTree.asyncEval(params);
      operator match {
      | Multiply() -> lhs * rhs
      | Divide() -> lhs / rhs
      | Add() -> lhs + rhs
      | Sub() -> lhs - rhs
      }
    | Negate(v) -> AF.negate(await v.asyncEval(params))
    | Reciprocal(v) -> AF.Const(1.0) / (await v.asyncEval(params))
    | Sin(v) -> AF.sin(await v.asyncEval(params))
    | Cos(v) -> AF.cos(await v.asyncEval(params))
    | Tan(v) -> AF.tan(await v.asyncEval(params))
    | Exp(v) -> AF.exp(await v.asyncEval(params))
    | Sigmoid(v) -> res = await v.asyncEval(params);
      AF.div_fa(1.0, AF.Const(1.0) + AF.exp(-res))
    | MatMul(u, v) ->
      lhs = await u.asyncEval(params);
      rhs = await v.asyncEval(params);
      AF.matmul(lhs, rhs)
    | Transpose(v) -> AF.transpose(await v.asyncEval(params))
    }
  }

  fun eval(params: readonly Params = Params()): AF.Array {
    awaitSynchronously(this.asyncEval(freeze(params.data)))
  }

  fun array(): AF.Array
  | Input(arr) -> arr
  | _ -> invariant_violation("Cannot extract array of symbolic tensor")
}

extension class Bind uses Show {
  fun toString(): String {
    this match {
    | Bind(name, _) -> name
    }
  }
}

extension class Param uses Show {
  fun toString(): String {
    this match {
    | Param(name) -> name
    }
  }
}

module end;


module Test;

extension class TestCase {
  mutable fun expect_array_equal(u: AF.Array, v: AF.Array): void {
    this.expect(AF.allTrue(AF.lt_av(AF.abs(u - v), 1E-5)),
                "Arrays not equal")
  }
}

module end;


module TestTensor;

untracked fun testAll(): void {
  params = Tensor.Params(Map[
    "x" => Tensor.Input(AF.randu(5)),
    "y" => Tensor.Input(AF.randu(5))
  ]);

  Test.group("Tensor.backward")
  .test("multiply", t -> {
    x = Tensor.Param("x");
    y = x * x;
    grads = y.backward(params);
    dx = grads["x"].eval(params);

    t.expect_array_equal(dx, AF.mult_ia(2, x.eval(params)))
  })
  .test("test_multiply_add", t -> {
    x = Tensor.Param("x");
    y = Tensor.Param("y");
    z = x * x + x * y + y * y;
    grads = z.backward(params);
    dx = grads["x"].eval(params);
    dy = grads["y"].eval(params);

    t.expect_array_equal(dx, AF.mult_ia(2, x.eval(params)) + y.eval(params));
    t.expect_array_equal(dy, AF.mult_ia(2, y.eval(params)) + x.eval(params))
  })
  .test("test_multiply_sub", t -> {
    x = Tensor.Param("x");
    y = Tensor.Param("y");
    z = x * x - x * y;
    grads = z.backward(params);
    dx = grads["x"].eval(params);
    dy = grads["y"].eval(params);

    t.expect_array_equal(dx, AF.mult_ia(2, x.eval(params)) - y.eval(params));
    t.expect_array_equal(dy, -x.eval(params))
  })
  .test("test_divide_add", t -> {
    x = Tensor.Param("x");
    y = Tensor.Param("y");
    z = x + x / y + y;
    grads = z.backward(params);
    dx = grads["x"].eval(params);
    dy = grads["y"].eval(params);

    t.expect_array_equal(dx, AF.plus_fa(1.0, AF.div_fa(1.0, y.eval(params))));
    t.expect_array_equal(
      dy,
      AF.minus_fa(1.0, (x.eval(params) / (y.eval(params) * y.eval(params)))),
    )
  })
  .test("test_multiply_add_scalar", t -> {
    x = Tensor.Param("x");
    y = Tensor.Param("y");
    z = x * Tensor.Int(2) + x * y + y;
    grads = z.backward(params);
    dx = grads["x"].eval(params);
    dy = grads["y"].eval(params);

    t.expect_array_equal(dx, AF.plus_fa(2.0, y.eval(params)));
    t.expect_array_equal(dy, AF.plus_fa(1.0, x.eval(params)))
  })
  .test("test_bind", t -> {
    x = Tensor.Param("x");
    y = Tensor.Param("y");
    z = x * Tensor.Int(2) + Tensor.Bind("myVar", x * y) + y;
    grads = z.backward(params);
    dx = grads["x"].eval(params);
    dy = grads["y"].eval(params);
    AF.print(grads["myVar"].eval(params));

    t.expect_array_equal(dx, AF.Const(2.0) + y.eval(params));
    t.expect_array_equal(dy, AF.Const(1.0) + x.eval(params))
  })
  .test("test_exp", t -> {
    x = Tensor.Param("x");
    y = Tensor.Exp(x);
    grads = y.backward(params);
    dx = grads["x"].eval(params);

    t.expect_array_equal(dx, AF.exp(x.eval(params)))
  })
  .test("test_sigmoid", t -> {
    x = Tensor.Param("x");
    y = Tensor.Sigmoid(x);
    grads = y.backward(params);
    dx = grads["x"].eval(params);

    sigx = AF.div_fa(1.0, AF.plus_fa(1.0, AF.exp(-x.eval(params))));
    t.expect_array_equal(dx, sigx * ((AF.minus_fa(1.0, sigx))));

    z = Tensor.Reciprocal(Tensor.Constant(1.0, 5) + Tensor.Exp(-x));
    grads2 = z.backward(params);
    dx2 = grads2["x"].eval(params);

    t.expect_array_equal(dx2, sigx * ((AF.minus_fa(1.0, sigx))))
  })
  .test("test_basic_arithmetic", t -> {
    x = Tensor.Bind("x", Tensor.Constant(1.0, 5));
    y = x * x * x * x;
    grads = y.backward();
    t.expect_array_equal(grads["x"].eval(params), Tensor.Constant(4.0, 5).eval());
    t.expect_array_equal(grads[x].eval(params), Tensor.Constant(4.0, 5).eval())
  })
  .test("test_higher_order_gradients", t -> {
    x = Tensor.Bind("x", Tensor.Constant(1.0, 5));
    y = x * x * x * x;

    grads = y.backward();
    t.expect_array_equal(grads[x].eval(), Tensor.Constant(4.0, 5).eval());

    grads2 = grads[x].backward();
    t.expect_array_equal(grads2[x].eval(), Tensor.Constant(12.0, 5).eval());

    grads3 = grads2[x].backward();
    t.expect_array_equal(grads3[x].eval(), Tensor.Constant(24.0, 5).eval());

    grads4 = grads3[x].backward();
    t.expect_array_equal(grads4[x].eval(), Tensor.Constant(24.0, 5).eval());

    grads5 = grads4[x].backward();
    t.expect(!grads5.data.containsKey("x"));
})
  .run()
}

module end;
