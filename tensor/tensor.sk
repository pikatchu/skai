module alias AF = ArrayFire;

module Tensor;

type Env = Map<String, AF.Array>;

base class OperationKind {
  children =
  | Multiply()
  | Divide()
  | Add()
  | Sub()
}

class Int(value: .Int) extends Tensor {
  fun *(rhs: Tensor): Tensor {
    ConstantOfInt(this.value, rhs) * rhs
  }

  fun +(rhs: Tensor): Tensor {
    ConstantOfInt(this.value, rhs) + rhs
  }

  fun -(rhs: Tensor): Tensor {
    ConstantOfInt(this.value, rhs) - rhs
  }

  fun /(rhs: Tensor): Tensor {
    ConstantOfInt(this.value, rhs) / rhs
  }

  fun negate(): Tensor {
    !this.value = -this.value;
    this
  }

  fun reciprocal(): Tensor {
    invariant_violation("Reciprocal on Int not supported")
  }
}

base class .Tensor {
  children =
  | Input(String)
  | Constant(Float, .Int)
  | ConstantOfInt(.Int, Tensor)
  | Operation(OperationKind, Tensor, Tensor)
  | Negate(Tensor)
  | Reciprocal(Tensor)
  | Exp(Tensor)
  | Sigmoid(Tensor)
  | MatMul(Tensor, Tensor)
  | Bind(String, Tensor)
  | Transpose(Tensor)
  | Sin(Tensor)
  | Cos(Tensor)
  | Tan(Tensor)

  overridable fun *(rhs: Tensor): Tensor {
    rhs match {
    | Int(n) -> this * ConstantOfInt(n, this)
    | _ -> Operation(Multiply(), this, rhs)
    }
  }

  overridable fun /(rhs: Tensor): Tensor {
    rhs match {
    | Int(n) -> this / ConstantOfInt(n, this)
    | _ -> Operation(Divide(), this, rhs)
    }
  }

  overridable fun +(rhs: Tensor): Tensor {
    rhs match {
    | Int(n) -> this + ConstantOfInt(n, this)
    | _ -> Operation(Add(), this, rhs)
    }
  }

  overridable fun -(rhs: Tensor): Tensor {
    rhs match {
    | Int(n) -> this - ConstantOfInt(n, this)
    | _ -> Operation(Sub(), this, rhs)
    }
  }

  overridable fun negate(): Tensor {
    Negate(this)
  }

  overridable fun reciprocal(): Tensor {
    Reciprocal(this)
  }

  fun dims(env: Env): Vector<.Int>
  | Int(_tensor) -> invariant_violation("Case not handled yet")
  | Input(name) -> if (env.containsKey(name)) {
      v = mutable Vector[];
      for (i in Range(0, AF.numDims(env[name]))) {
        v.push(AF.getDim(env[name], i))
      };
      freeze(v)
    } else {
      invariant_violation(`Unknown input ${name}`)
    }
  | Constant(_, n) -> Vector[n]
  | Bind(_, tensor)
  | ConstantOfInt(_, tensor)
  | Negate(tensor)
  | Reciprocal(tensor)
  | Sin(tensor)
  | Cos(tensor)
  | Tan(tensor)
  | Exp(tensor)
  | Sigmoid(tensor) -> tensor.dims(env)
  | Transpose(tensor) -> d = tensor.dims(env);
  if (d.size() == 1) {
    if (d[0] == 1) {
      Vector[1]
    } else {
      Vector[1, d[0]]
    }
  } else if(d.size() == 2) {
    if (d[0] == 1) {
      Vector[d[1]]
    } else {
      Vector[d[1], d[0]]
    }
  } else {
    invariant_violation("Cannot transpose higher order tensors.")
  }
  | MatMul(u, v) -> du = u.dims(env); dv = v.dims(env);
  if (du.size() > 2 && dv.size() > 2) {
    invariant_violation("Cannot matmul higher order tensors.")
  };
  if ((du.size() == 1 && dv[0] != 1) || (du.size() >= 2 && du[1] != dv[0])) {
    debug(u);
    debug(v);
    invariant_violation(`Dimensions mismatch: ${du} vs ${dv}`)
  };
  if (dv.size() >= 2) {
    Vector[du[0], dv[1]]
  } else {
    Vector[du[0]]
  }
  | Operation(_, lhs, rhs) -> lhsDims = lhs.dims(env); rhsDims = rhs.dims(env);
  if (lhsDims != rhsDims) {
    debug(lhs);
    debug(rhs);
    invariant_violation(`Dimensions mismatch: ${lhsDims} vs ${rhsDims}`)
  } else {
    lhsDims
  }


  static fun addGradient(
    name: String,
    gradient: Tensor,
    acc: mutable Map<String, Tensor>,
  ): void {
    acc![name] = if (acc.containsKey(name)) acc[name] + gradient else gradient
  }

  protected fun buildGradients(
    gradient: Tensor,
    acc: mutable Map<String, Tensor>,
  ): void
  | Int _
  | Constant _
  | ConstantOfInt _ ->
    void
  | Input(name) -> static::addGradient(name, gradient, acc)
  | Bind(name, tensor) ->
    static::addGradient(name, gradient, acc);
    tensor.buildGradients(gradient, acc)
  | Operation(operator, lhs, rhs) ->
    operator match {
    | Multiply() ->
      lhs.buildGradients(gradient * rhs, acc);
      rhs.buildGradients(gradient * lhs, acc)
    | Divide() ->
      rhsRec = rhs.reciprocal();
      lhsGrad = gradient * rhsRec;
      lhs.buildGradients(lhsGrad, acc);
      rhs.buildGradients(-lhsGrad * lhs * rhsRec, acc)
    | Add() ->
      lhs.buildGradients(gradient, acc);
      rhs.buildGradients(gradient, acc)
    | Sub() ->
      lhs.buildGradients(gradient, acc);
      rhs.buildGradients(-gradient, acc)
    }
  | Negate(tensor) -> tensor.buildGradients(-gradient, acc)
  | Reciprocal(tensor) -> res = tensor.reciprocal();
    tensor.buildGradients(-gradient * res * res, acc)
  | Sin(tensor) -> tensor.buildGradients(gradient * Cos(tensor), acc)
  | Cos(tensor) -> tensor.buildGradients(-gradient * Sin(tensor), acc)
  | Tan(tensor) -> tensor.buildGradients(
      gradient * Reciprocal(Cos(tensor) * Cos(tensor)), acc)
  | Exp(tensor) -> tensor.buildGradients(gradient * Exp(tensor), acc)
  | Sigmoid(tensor) -> res = Reciprocal(ConstantOfInt(1, tensor) + Exp(-tensor));
    tensor.buildGradients(gradient * res * (ConstantOfInt(1, res) - res), acc)
  | MatMul(lhs, rhs) ->
    lhs.buildGradients(MatMul(gradient, Transpose(rhs)), acc); // transpose lhs/rhs?
    rhs.buildGradients(MatMul(Transpose(lhs), gradient), acc)
  | Transpose(tensor) -> tensor.buildGradients(Transpose(gradient), acc)

  fun backward(gradient: Tensor): Map<String, Tensor> {
    acc = mutable Map[];
    this.buildGradients(gradient, acc);
    freeze(acc)
  }

  @gc
  memoized async fun asyncEval(env: Env): ^AF.Array {
    this match {
    | Int _ -> invariant_violation("Bad usage of Tensor.Int")
    | Input(n) ->
      if (!env.containsKey(n)) {
        invariant_violation(`Environment does not define the input ${n}`)
      } else {
        env[n]
      }
    | Bind(_, tensor) -> await tensor.asyncEval(env)
    | ConstantOfInt(value, tensor) ->
      AF.constant_of_int(value, await tensor.asyncEval(env))
    | Constant(value, dims) -> AF.constant(value, dims)
    | Operation(operator, lhsTree, rhsTree) ->
      lhs = await lhsTree.asyncEval(env);
      rhs = await rhsTree.asyncEval(env);
      operator match {
      | Multiply() -> AF.mult_aa(lhs, rhs)
      | Divide() -> AF.div_aa(lhs, rhs)
      | Add() -> AF.plus_aa(lhs, rhs)
      | Sub() -> AF.minus_aa(lhs, rhs)
      }
    | Negate(v) -> AF.negate(await v.asyncEval(env))
    | Reciprocal(v) -> AF.div_fa(1.0, await v.asyncEval(env))
    | Sin(v) -> AF.sin(await v.asyncEval(env))
    | Cos(v) -> AF.cos(await v.asyncEval(env))
    | Tan(v) -> AF.tan(await v.asyncEval(env))
    | Exp(v) -> AF.exp(await v.asyncEval(env))
    | Sigmoid(v) -> res = await v.asyncEval(env);
      AF.div_fa(1.0, AF.plus_fa(1.0, AF.exp(-res)))
    | MatMul(u, v) -> AF.matmul(await u.asyncEval(env), await v.asyncEval(env))
    | Transpose(v) -> AF.transpose(await v.asyncEval(env))
    }
  }

  fun eval(env: Env): AF.Array {
    awaitSynchronously(this.asyncEval(env))
  }
}

module end;


module Test;

extension class TestCase {
  mutable fun expect_array_equal(u: AF.Array, v: AF.Array): void {
    this.expect(AF.allTrue(AF.lt_av(AF.abs(u - v), 1E-5)),
                "Arrays not equal")
  }
}

module end;


module TestTensor;

untracked fun testAll(): void {
  env = Map["x" => AF.randu(5), "y" => AF.randu(5)];

  Test.group("Tensor.backward")
  .test("multiply", t -> {
    x = Tensor.Input("x");
    y = x * x;
    dy = Tensor.Constant(1.0, 5);
    grads = y.backward(dy);
    dx = grads["x"];

    t.expect_array_equal(dx.eval(env), AF.mult_ia(2, x.eval(env)))
  })
  .test("test_multiply_add", t -> {
    x = Tensor.Input("x");
    y = Tensor.Input("y");
    z = x * x + x * y + y * y;
    dz = Tensor.Constant(1.0, 5);
    grads = z.backward(dz);
    dx = grads["x"];
    dy = grads["y"];

    t.expect_array_equal(dx.eval(env), AF.mult_ia(2, x.eval(env)) + y.eval(env));
    t.expect_array_equal(dy.eval(env), AF.mult_ia(2, y.eval(env)) + x.eval(env))
  })
  .test("test_multiply_sub", t -> {
    x = Tensor.Input("x");
    y = Tensor.Input("y");
    z = x * x - x * y;
    dz = Tensor.Constant(1.0, 5);
    grads = z.backward(dz);
    dx = grads["x"];
    dy = grads["y"];

    t.expect_array_equal(dx.eval(env), AF.mult_ia(2, x.eval(env)) - y.eval(env));
    t.expect_array_equal(dy.eval(env), -x.eval(env))
  })
  .test("test_divide_add", t -> {
    x = Tensor.Input("x");
    y = Tensor.Input("y");
    z = x + x / y + y;
    dz = Tensor.Constant(1.0, 5);
    grads = z.backward(dz);
    dx = grads["x"];
    dy = grads["y"];

    t.expect_array_equal(dx.eval(env), AF.plus_fa(1.0, AF.div_fa(1.0, y.eval(env))));
    t.expect_array_equal(
      dy.eval(env),
      AF.minus_fa(1.0, (x.eval(env) / (y.eval(env) * y.eval(env)))),
    )
  })
  .test("test_multiply_add_scalar", t -> {
    x = Tensor.Input("x");
    y = Tensor.Input("y");
    z = x * Tensor.Int(2) + x * y + y;
    dz = Tensor.Constant(1.0, 5);
    grads = z.backward(dz);
    dx = grads["x"];
    dy = grads["y"];

    t.expect_array_equal(dx.eval(env), AF.plus_fa(2.0, y.eval(env)));
    t.expect_array_equal(dy.eval(env), AF.plus_fa(1.0, x.eval(env)))
  })
  .test("test_bind", t -> {
    x = Tensor.Input("x");
    y = Tensor.Input("y");
    z = x * Tensor.Int(2) + Tensor.Bind("myVar", x * y) + y;
    dz = Tensor.Constant(1.0, 5);
    grads = z.backward(dz);
    dx = grads["x"];
    dy = grads["y"];
    AF.print(grads["myVar"].eval(env));

    t.expect_array_equal(dx.eval(env), AF.plus_fa(2.0, y.eval(env)));
    t.expect_array_equal(dy.eval(env), AF.plus_fa(1.0, x.eval(env)))
  })
  .test("test_exp", t -> {
    x = Tensor.Input("x");
    y = Tensor.Exp(x);
    dy = Tensor.Constant(1.0, 5);
    grads = y.backward(dy);
    dx = grads["x"];

    t.expect_array_equal(dx.eval(env), AF.exp(x.eval(env)))
  })
  .test("test_sigmoid", t -> {
    x = Tensor.Input("x");
    y = Tensor.Sigmoid(x);
    dy = Tensor.Constant(1.0, 5);
    grads = y.backward(dy);
    dx = grads["x"];

    sigx = AF.div_fa(1.0, AF.plus_fa(1.0, AF.exp(-x.eval(env))));
    t.expect_array_equal(dx.eval(env), sigx * ((AF.minus_fa(1.0, sigx))))
  }).run()
}

module end;
