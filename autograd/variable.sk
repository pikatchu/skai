module alias AF = ArrayFire;

type Dag = Vector<mutable Variable>;
type GradFunc = (mutable Vector<mutable Variable>, mutable Variable) -> void;

class IMat(value: Int) {
  fun *(rhs: mutable Variable): mutable Variable {
    cst = AF.constant_of_int(this.value, rhs.array());
    lhs = Variable::new(cst, false);
    lhs * rhs
  }

  fun +(rhs: mutable Variable): mutable Variable {
    cst = AF.constant_of_int(this.value, rhs.array());
    lhs = Variable::new(cst, false);
    lhs + rhs
  }
}

class FMat(value: Float) {
  fun *(rhs: mutable Variable): mutable Variable {
    cst = AF.constant_of_float(this.value, rhs.array());
    lhs = Variable::new(cst, false);
    lhs * rhs
  }

  fun +(rhs: mutable Variable): mutable Variable {
    cst = AF.constant_of_float(this.value, rhs.array());
    lhs = Variable::new(cst, false);
    lhs + rhs
  }
}

mutable class Variable{
  data: AF.Array,
  inputs: mutable Vector<mutable Variable> = mutable Vector[],
  mutable gradFunc: ?(GradFunc) = None(),
  grads: mutable Vector<mutable Variable> = mutable Vector[],
  mutable calcGrad: Bool = false,
} {
  type ID = Int;

  static fun new(data: AF.Array, calcGrad: Bool): mutable this {
    mutable static{data, calcGrad}
  }

  readonly fun array(): AF.Array {
    this.data
  }

  mutable fun addGrad(variable: mutable Variable): void {
    this.grads.push(variable)
  }

  mutable fun grad(): mutable Variable {
    if (!this.calcGrad) {
      invariant_violation("Gradient calculation disabled.");
    };
    if (this.grads.size() == 0) {
      invariant_violation("Gradient hasn't been calculated yet.");
    };
    this.grads[0];
  }

  mutable fun negate(): mutable Variable {
    data = AF.minus_fa(0.0, this.array());
    gradFunc = Some((inputs, gradOutput) -> {
      inputs[0].addGrad(gradOutput.negate());
    });
    inputs = mutable Vector[this];
    mutable Variable{data, inputs, gradFunc};
  }

  mutable fun reciprocal(): mutable Variable {
    data = AF.div_fa(1.0, this.array());
    gradFunc = Some((inputs, grad_output) -> {
      res = inputs[0].reciprocal();
      inputs[0].addGrad(grad_output.negate() * res * res);
    });
    inputs = mutable Vector[this];
    mutable Variable{data, inputs, gradFunc};
  }

  mutable fun +(rhs: mutable Variable): mutable Variable {
    data = AF.plus_aa(this.array(), rhs.array());
    gradFunc = Some((inputs, gradOutput) -> {
      inputs[0].addGrad(gradOutput);
      inputs[1].addGrad(gradOutput);
    });
    inputs = mutable Vector[this, rhs];
    mutable Variable{data, inputs, gradFunc};
  }

  mutable fun -(rhs: mutable Variable): mutable Variable {
    data = AF.minus_aa(this.array(), rhs.array());
    gradFunc = Some((inputs, grad_output) -> {
      inputs[0].addGrad(grad_output);
      inputs[1].addGrad(grad_output.negate());
    });
    inputs = mutable Vector[this, rhs];
    mutable Variable{data, inputs, gradFunc};
  }

  mutable fun *(rhs: mutable Variable): mutable Variable {
    data = AF.mult_aa(this.array(), rhs.array());
    gradFunc = Some((inputs, grad_output) -> {
      inputs[0].addGrad(grad_output * inputs[1]);
      inputs[1].addGrad(grad_output * inputs[0]);
    });
    inputs = mutable Vector[this, rhs];
    mutable Variable{data, inputs, gradFunc};
  }

  mutable fun /(rhs: mutable Variable): mutable Variable {
    data = this.array() / rhs.array();
    gradFunc = Some((inputs, grad_output) -> {
      inputs_1_rec = inputs[1].reciprocal();
      grad_input_0 = grad_output * inputs_1_rec;
      inputs[0].addGrad(grad_input_0);
      inputs[1].addGrad(grad_input_0 * inputs[0].negate() * inputs_1_rec);
    });
    inputs = mutable Vector[this, rhs];
    mutable Variable{data, inputs, gradFunc};
  }

  mutable fun setCalcGrad(calcGrad: Bool): void {
    this.!calcGrad = calcGrad;
    if (!calcGrad) {
      this.!gradFunc = None();
      this.inputs.clear();
      this.grads.clear();
    }
  }

  mutable fun evalGrad(retainGradGraph: Bool): void {
    // Flag asking not to calculate gradients
    if (!this.calcGrad) return void;

    // Best not to evaluate the JIT immediately if theres only a single gradient
    grad = this.grads[0];
    if (this.grads.size() > 1) {
      for (i in Range(1, this.grads.size())) {
        !grad = grad + this.grads[i];
      };
      AF.eval(grad.array());
      this.grads.resize(1, grad);
    };

    grad.setCalcGrad(retainGradGraph);
    this.grads![0] = grad;
  }

  mutable fun calcGradInputs(retainGradGraph: Bool): void {
    this.evalGrad(retainGradGraph);
    this.gradFunc match {
    | None() -> void
    | Some(gradFun) -> gradFun(this.inputs, this.grads[0])
    }
  }

  mutable fun backward(
    grad: mutable Variable = mutable Variable{
      data => AF.constant_one(this.array()),
      calcGrad => false,
    },
    retainGradGraph: Bool = false,
  ): void {
    this.addGrad(grad);
    dag = Variable::build(this);
    for (iter in dag) {
      iter.calcGradInputs(retainGradGraph);
    }
  }

  static fun build(var: mutable Variable): mutable Dag {
    cache = Set<Variable::ID>::mcreate();
    dag = mutable Vector[];
    Variable::buildSubGraph(cache, dag, var);
    dag
  }

  static fun buildSubGraph(
    cache: mutable Set<Variable::ID>,
    dag: mutable Dag,
    var: mutable Variable,
  ): void {
    dag.push(var);
    /*    id = var.id();
        if (cache.find(id) != cache.end()) {
          return void;
        };
    */
    for (input in var.inputs) {
      Variable::buildSubGraph(cache, dag, input);
    };
    //    cache![id] = true;
  }
}

module TestVariable;
fun verify(v: AF.Array): void {
  res = AF.allTrue(AF.lt_av(AF.abs(v), 1E-5));
  if (res) print_string("PASSED") else print_string("FAILED");
}

untracked fun test_multiply(): void {
  x = Variable::new(AF.randu(5), true);
  y = x * x;
  dy = Variable::new(AF.constant(1.0, 5), false);
  y.backward(dy);
  dx = x.grad();
  arr = dx.array() - AF.mult_ia(2, x.array());
  verify(arr);
}

untracked fun test_multiply_add(): void {
  x = Variable::new(AF.randu(5), true);
  y = Variable::new(AF.randu(5), true);
  z = x * x + x * y + y * y;
  dz = Variable::new(AF.constant(1.0, 5), false);
  z.backward(dz);
  dx = x.grad();
  dy = y.grad();
  verify(dx.array() - AF.mult_ia(2, x.array()) - y.array());
  verify(dy.array() - AF.mult_ia(2, y.array()) - x.array());
}

untracked fun test_no_calc_grad(): void {
  x = Variable::new(AF.randu(5), false);
  y = Variable::new(AF.randu(5), true);
  z = x * x + x * y + y * y;
  dz = Variable::new(AF.constant(1.0, 5), false);
  z.backward(dz);
  dy = y.grad();
  verify(dy.array() - AF.mult_ia(2, y.array()) - x.array());
  try {
    _dx = x.grad();
  } catch {
  | _ -> return void
  };
  invariant_violation("No Gradient check Failed\n");
}

untracked fun test_multiply_sub(): void {
  x = Variable::new(AF.randu(5), true);
  y = Variable::new(AF.randu(5), true);
  z = x * x - x * y;
  dz = Variable::new(AF.constant(1.0, 5), false);
  z.backward(dz);
  dx = x.grad();
  dy = y.grad();
  verify(dx.array() - (AF.mult_ia(2, x.array()) - y.array()));
  verify(dy.array() - (-x.array()));
}

untracked fun test_divide_add(): void {
  x = Variable::new(AF.randu(5), true);
  y = Variable::new(AF.randu(5), true);
  z = x + x / y + y;
  dz = Variable::new(AF.constant(1.0, 5), false);
  z.backward(dz);
  dx = x.grad();
  dy = y.grad();
  verify(dx.array() - (AF.plus_fa(1.0, AF.div_fa(1.0, y.array()))));
  verify(
    dy.array() - (AF.minus_fa(1.0, (x.array() / (y.array() * y.array())))),
  );
}

untracked fun test_multiply_add_scalar(): void {
  x = Variable::new(AF.randu(5), true);
  y = Variable::new(AF.randu(5), true);
  z = IMat(2) * x + x * y + y;
  dz = Variable::new(AF.constant(1.0, 5), false);
  z.backward(dz);
  dx = x.grad();
  dy = y.grad();
  verify(dx.array() - (AF.plus_fa(2.0, y.array())));
  verify(dy.array() - (AF.plus_fa(1.0, x.array())));
}

untracked fun testAll(): void {
  test_multiply();
  test_multiply();
  test_multiply_add();
  test_no_calc_grad();
  test_multiply_sub();
  test_divide_add();
  test_multiply_add_scalar();
}

module end;
